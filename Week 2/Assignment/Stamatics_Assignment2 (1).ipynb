{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5y_HbJiPKhA"
      },
      "source": [
        "# Question 1\n",
        "## Regularization in Linear Regression\n",
        "Make a class called LinearRegression which provides two functions : fit and predict. You may use the code present in the repository as template. You need to make the following changes in order to include regularization :\n",
        "1. To initialize an object of the class, you need to provide 4 parameters : learning_rate, epochs, penalty and alpha (coefficient of the regularization term). Penalty and alpha should have default values of None and 0 respectively.\n",
        "2. The parameter penalty should take in any one of these inputs : L1 (Lasso), L2 (Ridge) and None (simple LR).\n",
        "3. Do some basic differentiation to find out the expressions of dC/dw and dC/db when regularization is involved. Use internet whenever necessary.\n",
        "4. Write if-else statements inside the fit method to cover the different values for dw for different values of penalty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gzoG2XilPLFr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class LinearRegression:\n",
        "    def __init__(self, learning_rate, epochs, penalty=None, alpha=0):\n",
        "        self.lr=learning_rate\n",
        "        self.epochs=epochs\n",
        "        self.penalty = penalty\n",
        "        self.alpha = alpha\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        n_samples, n_features = X_train.shape\n",
        "        y_train=y_train.reshape(-1,1)\n",
        "        # init parameters\n",
        "        self.weights = np.zeros((n_features,1))\n",
        "        self.bias = np.zeros((1,1))\n",
        "\n",
        "        # gradient descent\n",
        "        for i in range(self.epochs):\n",
        "            y_pred = self.predict(X_train)\n",
        "            if self.penalty is None:\n",
        "                dw = (2 / num_samples) * np.dot(X.T, (y_pred - y))\n",
        "            elif self.penalty == 'L1':\n",
        "                dw = (2 / num_samples) * np.dot(X.T, (y_pred - y)) + (self.alpha / num_samples) * np.sign(self.weights)\n",
        "            elif self.penalty == 'L2':\n",
        "                dw = (2 / num_samples) * np.dot(X.T, (y_pred - y)) + (2 * self.alpha / num_samples) * self.weights\n",
        "\n",
        "            #update weights and biases\n",
        "            self.weights-= self.lr * dw\n",
        "            self.bias-= self.lr* db\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        y_predicted = np.dot(X_test,self.weights)+self.bias\n",
        "        print(self.weights, self.bias)\n",
        "        return y_predicted"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PsqoxNag7D3-"
      },
      "source": [
        "# Question 2\n",
        "# Linear Regression\n",
        "Use the dataset https://www.kaggle.com/datasets/quantbruce/real-estate-price-prediction (*).\n",
        "1. Read it using pandas.\n",
        "2. Check for **null values**.\n",
        "3. For each of the columns (except the first and last), plot the column values in the X-axis against the last column of prices in the Y-axis.\n",
        "4. Remove the unwanted columns.\n",
        "5. Split the dataset into train and test data. Test data size = 25% of total dataset.\n",
        "6. **Normalize** the X_train and X_test using MinMaxScaler from sklearn.preprocessing.\n",
        "7. Fit the training data into the 3 models created in question 1 (**linear regression, lasso and ridge regression**) and predict the testing data.\n",
        "8. Use **mean square error and R<sup>2</sup>** from sklearn.metrics as evaluation criterias.\n",
        "9. Fit the training data into the models of the same name provided by sklearn.linear_model and evaluate the predictions using MSE and R<sup>2</sup>.\n",
        "10. Tune the hyperparameters of your models (learning rate, epochs, penalty and alpha) to achieve losses close to that of the sklearn models. (*We will cover hyperparameter tuning using GridSearchCV and all in later weeks. For now, you may manually run the model for different values of the hyperparameters.*)\n",
        "\n",
        "Note : (*) To solve this question, you may proceed in any of the following ways :\n",
        "1. Prepare the notebook in Kaggle, download it and submit it separately with the other questions.\n",
        "2. Download the dataset from kaggle. Upload it to the session storage in Colab.\n",
        "3. Use Colab data directly in Colab. [Refer here](https://www.kaggle.com/general/74235). For this, you need to create kaggle API token. Before submitting, hide or remove the API token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8lupaMcr63QF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Real estate.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7832\\3629346761.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Step 1: Read the dataset using pandas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Real estate.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Step 2: Check for null values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Bikas K Kundu\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Bikas K Kundu\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Bikas K Kundu\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Bikas K Kundu\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Bikas K Kundu\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Bikas K Kundu\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Bikas K Kundu\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m             \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"encoding_errors\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"strict\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m         )\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Bikas K Kundu\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 707\u001b[1;33m                 \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    708\u001b[0m             )\n\u001b[0;32m    709\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Real estate.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Step 1: Read the dataset using pandas\n",
        "df = pd.read_csv('Real estate.csv')\n",
        "\n",
        "# Step 2: Check for null values\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Step 3: Plot each column against the last column (prices)\n",
        "last_column = df.columns[-1]\n",
        "for column in df.columns[1:-1]:\n",
        "    plt.scatter(df[column], df[last_column])\n",
        "    plt.xlabel(column)\n",
        "    plt.ylabel(last_column)\n",
        "    plt.show()\n",
        "\n",
        "# Step 4: Remove unwanted columns\n",
        "unwanted_columns = [df.columns[0], last_column]\n",
        "df = df.drop(unwanted_columns, axis=1)\n",
        "\n",
        "# Step 5: Split the dataset into train and test data\n",
        "X = df.iloc[:, :-1]\n",
        "y = df.iloc[:, -1]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Step 6: Normalize the X_train and X_test using MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 7: Fit and predict using custom LinearRegression, Lasso, and Ridge models\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train_scaled, y_train)\n",
        "y_pred_lr = lr.predict(X_test_scaled)\n",
        "\n",
        "lasso = Lasso(alpha=0.1)\n",
        "lasso.fit(X_train_scaled, y_train)\n",
        "y_pred_lasso = lasso.predict(X_test_scaled)\n",
        "\n",
        "ridge = Ridge(alpha=0.1)\n",
        "ridge.fit(X_train_scaled, y_train)\n",
        "y_pred_ridge = ridge.predict(X_test_scaled)\n",
        "\n",
        "# Step 8: Evaluate predictions using mean squared error (MSE) and R2 score\n",
        "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
        "r2_lr = r2_score(y_test, y_pred_lr)\n",
        "print(\"Linear Regression:\")\n",
        "print(\"Mean Squared Error:\", mse_lr)\n",
        "print(\"R2 Score:\", r2_lr)\n",
        "print()\n",
        "\n",
        "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
        "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
        "print(\"Lasso Regression:\")\n",
        "print(\"Mean Squared Error:\", mse_lasso)\n",
        "print(\"R2 Score:\", r2_lasso)\n",
        "print()\n",
        "\n",
        "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
        "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
        "print(\"Ridge Regression:\")\n",
        "print(\"Mean Squared Error:\", mse_ridge)\n",
        "print(\"R2 Score:\", r2_ridge)\n",
        "print()\n",
        "\n",
        "# Step 9: Fit and predict using sklearn LinearRegression, Lasso, and Ridge models\n",
        "sk_lr = LinearRegression()\n",
        "sk_lr.fit(X_train_scaled, y_train)\n",
        "y_pred_sk_lr = sk_lr.predict(X_test_scaled)\n",
        "\n",
        "sk_lasso = Lasso(alpha=0.1)\n",
        "sk_lasso.fit(X_train_scaled, y_train)\n",
        "y_pred_sk_lasso = sk_lasso.predict(X_test_scaled)\n",
        "\n",
        "sk_ridge = Ridge(alpha=0.1)\n",
        "sk_ridge.fit(X_train_scaled, y_train)\n",
        "y_pred_sk_ridge = sk_ridge.predict(X_test_scaled)\n",
        "\n",
        "# Step 10: Evaluate sklearn model predictions using MSE and R2 score\n",
        "mse_sk_lr = mean_squared_error(y_test, y_pred_sk_lr)\n",
        "r2_sk_lr = r2_score(y_test, y_pred_sk_lr)\n",
        "print(\"Sklearn Linear Regression:\")\n",
        "print(\"Mean Squared Error:\", mse_sk_lr)\n",
        "print(\"R2 Score:\", r2_sk_lr)\n",
        "print()\n",
        "\n",
        "mse_sk_lasso = mean_squared_error(y_test, y_pred_sk_lasso)\n",
        "r2_sk_lasso = r2_score(y_test, y_pred_sk_lasso)\n",
        "print(\"Sklearn Lasso Regression:\")\n",
        "print(\"Mean Squared Error:\", mse_sk_lasso)\n",
        "print(\"R2 Score:\", r2_sk_lasso)\n",
        "print()\n",
        "\n",
        "mse_sk_ridge = mean_squared_error(y_test, y_pred_sk_ridge)\n",
        "r2_sk_ridge = r2_score(y_test, y_pred_sk_ridge)\n",
        "print(\"Sklearn Ridge Regression:\")\n",
        "print(\"Mean Squared Error:\", mse_sk_ridge)\n",
        "print(\"R2 Score:\", r2_sk_ridge)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dQ7lQpy-SYCq"
      },
      "source": [
        "# Question 3\n",
        "## Logistic Regression\n",
        "The breast cancer dataset is a binary classification dataset commonly used in machine learning tasks. It is available in scikit-learn (sklearn) as part of its datasets module.\n",
        "Here is an explanation of the breast cancer dataset's components:\n",
        "\n",
        "* Features (X):\n",
        "\n",
        " * The breast cancer dataset consists of 30 numeric features representing different characteristics of the FNA images. These features include mean, standard error, and worst (largest) values of various attributes such as radius, texture, smoothness, compactness, concavity, symmetry, fractal dimension, etc.\n",
        "\n",
        "* Target (y):\n",
        "\n",
        " * The breast cancer dataset is a binary classification problem, and the target variable (y) represents the diagnosis of the breast mass. It contains two classes:\n",
        "    * 0: Represents a malignant (cancerous) tumor.\n",
        "    * 1: Represents a benign (non-cancerous) tumor.\n",
        "\n",
        "Complete the code given below in place of the \"...\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "auipk-zBpmO-"
      },
      "source": [
        "1. Load the dataset from sklearn.datasets\n",
        "2. Separate out the X and Y columns.\n",
        "3. Normalize the X data using MinMaxScaler or StandardScaler.\n",
        "4. Create a train-test-split. Take any suitable test size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0OyGNHNjFh13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape: (353, 10)\n",
            "X_test shape: (89, 10)\n",
            "y_train shape: (353,)\n",
            "y_test shape: (89,)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Step 1: Load the dataset from sklearn.datasets\n",
        "dataset = load_diabetes()\n",
        "\n",
        "# Step 2: Separate the features (X) and the target variable (y)\n",
        "X = dataset.data\n",
        "y = dataset.target\n",
        "\n",
        "# Step 3: Normalize the X data using MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_normalized = scaler.fit_transform(X)\n",
        "\n",
        "# Step 4: Create a train-test-split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check the shapes of the train and test data\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uM-SsSxpqF2o"
      },
      "source": [
        "5. Write code for the sigmoid function and Logistic regression.\n",
        "(Logistic Regression code is available in the Week2/Examples folder. However, try to code it yourself. A template is provided for that.)\n",
        "\n",
        "*Optional* : Include the regularization terms as you did in the first question. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "o81LA5MZFoTW"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "   a=1.0/(1.0+ np.exp(-z))\n",
        "   return a\n",
        "\n",
        "def sigmoid_derivative(z):\n",
        "    return self.sigmoid(z)*(1-self.sigmoid(z))\n",
        "\n",
        "class LogisticRegression:\n",
        "    def __init__(self, learning_rate, epochs):\n",
        "      #Initialise the hyperparameters of the model\n",
        "        self.lr = learning_rate\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape \n",
        "        y = y.reshape(-1, 1)\n",
        "        self.weights = np.random.randn(n_features,1)/np.sqrt(n_features)\n",
        "        self.bias = np.random.randn(1,1)\n",
        "\n",
        "        #Implement the GD algortihm\n",
        "        for _ in range(self.epochs):\n",
        "            z = np.dot(X,self.weights) + self.bias\n",
        "            y_pred = sigmoid(z)\n",
        "\n",
        "            dw = -np.dot(X.T,(y - y_pred))/n_samples\n",
        "            db = -np.sum(y - y_pred)/n_samples\n",
        "\n",
        "            self.weights -= self.lr* dw\n",
        "            self.bias -= self.lr* db\n",
        "\n",
        "    def predict(self, X):\n",
        "      y_pred = np.dot(X,self.weights)+self.bias\n",
        "\n",
        "      for i in range(len(y_pred)):\n",
        "            if y_pred[i]<= 0.5:\n",
        "                y_pred[i] = 0\n",
        "            else:\n",
        "                y_pred[i] = 1\n",
        "      return y_pred"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Uo9LNRMzq4K-"
      },
      "source": [
        "6. Fit your model on the dataset and make predictions.\n",
        "7. Compare your model with the Sklearn Logistic Regression model. Try out all the different penalties.\n",
        "8. Print accuracy_score in each case using sklearn.metrics ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DyGsTTOqFphf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.93\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate classification data\n",
        "X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
        "\n",
        "# Create an instance of LogisticRegression\n",
        "logreg = LogisticRegression()\n",
        "\n",
        "# Fit the model\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Predict the output\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix,classification_report\n",
        "print(accuracy_score(y_test,y_pred))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AGBkzAO5red4"
      },
      "source": [
        "9. For the best model in each case (yours and scikit-learn), print the classification_report using sklearn.metrics .\n",
        "10. For the best model in each case (yours and scikit-learn), print the confusion_matrix using sklearn.metrics .\n",
        "11. **Optional Challenge** : For the best model in each case (yours and scikit-learn), print the roc_auc_score and plot the roc curves using sklearn.metrics and matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "le-HfABsvnyF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.96      0.94        53\n",
            "           1       0.95      0.89      0.92        47\n",
            "\n",
            "    accuracy                           0.93       100\n",
            "   macro avg       0.93      0.93      0.93       100\n",
            "weighted avg       0.93      0.93      0.93       100\n",
            "\n",
            "[[51  2]\n",
            " [ 5 42]]\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_test,y_pred))\n",
        "print(confusion_matrix(y_test,y_pred))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6OQ2tSp0MO6n"
      },
      "source": [
        "# Question 4\n",
        "## KNN and Decision Tree\n",
        "How accurately can a K-Nearest Neighbors (KNN) model classify different types of glass based on a glass classification dataset consisting of 214 samples and 7 classes? Use the kaggle dataset \"https://www.kaggle.com/datasets/uciml/glass\". \n",
        "\n",
        "Context: This is a Glass Identification Data Set from UCI. It contains 10 attributes including id. The response is glass type(discrete 7 values)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iMGxbtX-zfsI"
      },
      "source": [
        "1. Load the data as you did in the 2nd question.\n",
        "2. Extract the X and Y columns.\n",
        "3. Split it into training and testing datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "p0SfLB7pO7_z"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'load_glass' from 'sklearn.datasets' (c:\\Users\\Bikas K Kundu\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\datasets\\__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7832\\480879878.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_glass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneighbors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKNeighborsRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mImportError\u001b[0m: cannot import name 'load_glass' from 'sklearn.datasets' (c:\\Users\\Bikas K Kundu\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\datasets\\__init__.py)"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_glass\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "dataset = load_glass()\n",
        "\n",
        "# Step 2: Extract the features (X) and target variable (y)\n",
        "X = dataset.data\n",
        "y = dataset.target\n",
        "\n",
        "# Step 3: Split the data into training and testing datasets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Normalize the data (optional)\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 5: Create and train the KNN model\n",
        "knn = KNeighborsRegressor(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 6: Predict on the testing dataset\n",
        "y_pred = knn.predict(X_test_scaled)\n",
        "\n",
        "# Step 7: Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R2 Score:\", r2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qtyZJXh9zoh5"
      },
      "source": [
        "4. Define Euclidean distance.\n",
        "5. Build the KNN model.\n",
        "6. Fit the model on the training data. (Note : you may require to change the type of the data from pandas dataframe to numpy arrays. To do that, just do this X=np.array(X) and so on...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YJkhLORLzn6r"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'sklearn.datasets' has no attribute 'load_glass'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7832\\827599854.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mcolormap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlcm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'red'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'blue'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'yellow'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mglass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_glass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglass\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mglass\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mAttributeError\u001b[0m: module 'sklearn.datasets' has no attribute 'load_glass'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "def euclidean_distance(x1,x2):\n",
        "    return np.sqrt(np.sum((x1-x2)**2))\n",
        "\n",
        "class KNN(object):\n",
        "    def __init__(self,k):\n",
        "        self.k=k\n",
        "    def fit(self,x_train,y_train):\n",
        "        self.x_train=x_train\n",
        "        self.y_train=y_train\n",
        "    def predict(self,x_test):\n",
        "        predictions=[self._helper(x) for x in x_test]\n",
        "        return np.array(predictions)\n",
        "    def _helper(self,x):\n",
        "        prediction=[euclidean_distance(x,x1) for x1 in self.x_train]\n",
        "        indices= np.argsort(prediction)[:self.k]\n",
        "        labels= [self.y_train[i] for i in indices]\n",
        "        c=Counter(labels).most_common()\n",
        "        return c[0][0]\n",
        "        \n",
        "def accuracy(predictions,y_test):\n",
        "    return np.sum(predictions==y_test)/len(y_test)\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap as lcm\n",
        "colormap=lcm(['red','blue','yellow'])\n",
        "\n",
        "glass = datasets.load_glass()\n",
        "x,y = glass.data,glass.target\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "E9rxZpPB0pVS"
      },
      "source": [
        "7. Make predictions. Find their accuracy using accuracy_score. Try different k values. k=3 worked well in our case.\n",
        "8. Compare with the sklearn model (from sklearn.neighbors import KNeighborsClassifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ernfjaZJ0pAh"
      },
      "outputs": [],
      "source": [
        "clf=KNN(k=3)\n",
        "clf.fit(x_train,y_train)\n",
        "predictions=clf.predict(x_test)\n",
        "print(accuracy(predictions,y_test))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XRI7CeJQ0-iP"
      },
      "source": [
        "9. Finally use sklearn.tree to implement a Decision Tree Classifier on this dataset. Check for max depth = 5 to 10. Find out its accuracy.\n",
        "10. Plot the decision tree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2tZQg4L09wn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
